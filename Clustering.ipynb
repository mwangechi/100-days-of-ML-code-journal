{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLUSTERING\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Cluster</strong> : a collection of data points aggregated together because of certain similarities\n",
    "\n",
    "<strong> Clustering</strong> : It is a form of unsupervised learning (a method that draws references from datasets consisting of input data that is not labeled)\n",
    "<p> It is the task of dividing the population or data points into a number of groups such that data points in that data points in the same group are more similar to other data points in the same group and dissimilar to the data points in other groups. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types of clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Each methodology has a way of defining similarity among data points</p>\n",
    "\n",
    "<h4> Connectivity models:</h4> these models are based on the notion that the data points closer in data space exhibit more similarity to each other than the data points lying farther away. They can follow two approaches. \n",
    "<p>In the first approach, they start with classifying all data points into separate clusters & then aggregating them as the distance decreases.</p>\n",
    "<p>In the second approach, all data points are classified as a single cluster and then partitioned as the distance increases. Also, the choice of distance function is subjective. These models are very easy to interpret but lacks scalability for handling big datasets. Examples of these models are hierarchical clustering algorithm and its variants.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Centroid models:</h4> <p>it's an iterative clustering algorithm <p>\n",
    "    <p> the notion of similarity is derived by the closeness of a data point to the centroid of the clusters.</p>\n",
    "    <p>K-Means clustering algorithm is a popular algorithm that falls into this category. <p>In these models, the no. of clusters required at the end have to be mentioned beforehand, which makes it important to have prior knowledge of the dataset. These models run iteratively to find the local optima.<p/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Distribution models:</h4> are based on the notion of how probable is it that all data points in the cluster belong to the same distribution (e.g: Normal, Gaussian). These models often suffer from overfitting. A popular example of these models is Expectation-maximization algorithm which uses multivariate normal distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Density Models:</h4>These models search the data space for areas of varied density of data points in the data space. It isolates various different density regions and assign the data points within these regions in the same cluster. Popular examples of density models are DBSCAN and OPTICS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> K Means Clustering</h3>\n",
    "it aims at finding a local maxima in each iteration\n",
    "\n",
    "<p> <strong>Steps</strong></p>\n",
    "<ol> \n",
    "    <li>Specify the desired number of clusters K </li>\n",
    "     <li>Randomly assign each data point to a cluster</li>\n",
    "     <li>Compute cluster centroids </li>\n",
    "    <li>Re-assign each point to the closest cluster centroid</li>\n",
    "    <li>Re-compute cluster centroids</li>\n",
    "    <li>Repeat steps 4 and 5 until no improvements are possible</li>\n",
    "</ol>\n",
    "<p><strong>Determining Optimal Number of Clusters</strong>\n",
    "    <li><strong> The “Elbow” Method</strong></li>\n",
    "    <p>the sum of squares at each number of clusters is calculated and graphed, and the user looks for a change of slope from steep to shallow (an elbow) to determine the optimal number of clusters. This method is inexact, but still potentially helpful.</p>\n",
    "    <li><strong> The Gap Statistic</strong></li>\n",
    "    <p>it compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic (i.e., that yields the largest gap statistic). This means that the clustering structure is far away from the random uniform distribution of points.</p>\n",
    "    <li><strong> The Silhouette Method</strong></li>\n",
    "    <p> Average silhouette method computes the average silhouette of observations for different values of k. The optimal number of clusters k is the one that maximize the average silhouette over a range of possible values for k.</p>\n",
    "    <li><strong>The Sum of Squares Method</strong></li>\n",
    "    <p>  choose the optimal number of cluster by minimizing the within-cluster sum of squares (a measure of how tight each cluster is) and maximizing the between-cluster sum of squares (a measure of how seperated each cluster is from the others).</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Hierachial Clustering</h3>\n",
    "<p>It builds hierachial clusters\n",
    "The algorithm starts with all the data points assigned each a cluster of their own.\n",
    "Then the nearest two clusters are merged into the nearest same cluster.\n",
    "The algorithm terminates when there's only a single cluster left. Different groups can be choosen by obeserving the dendogram.</p>\n",
    "<p>Best choice of the number of clusters is on the number of vertical lines in the dendogram cut by a horizontal line that transverses the maximum distance vertically without intersecting a cluster.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Metrics for deciding the closeness of two clusters</h6>\n",
    "<ol>\n",
    "    <li>Euclidean distance</li> : computes the root of square difference\n",
    "between co-ordinates of pair of objects. \n",
    "    <li>Squared euclidean distance</li>\n",
    "    <li>Manhattan distance</li> : the absolute differences between coordinates of pair of objects\n",
    "    <li>Maximum distance(Chebychev Distance)</li>: distance and is computed as the absolute magnitude of the\n",
    "differences between coordinate of a pair of objects.\n",
    "    <li>Mahalanobis distance</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Gaussian Mixture Model Clustering </h3>\n",
    "<p> A clustering algorithm that assumes that each cluster follows a certain statistical distribution</p>\n",
    "<p>Step</p>\n",
    "<ol>\n",
    "    <li>Initialize K gaussian distributuions</li>\n",
    "    <li>Soft cluster data</li>\n",
    "    <li>Re-estimate the gaussians (Maximization)</li>\n",
    "    <li>Evaluate log-likelihood check for convergence</li>\n",
    "    <li>Repeat step 2 until converged</li>\n",
    "    </ol>\n",
    "    <p>Advantages</p>\n",
    "    <ol>\n",
    "    <li>Soft clustering (sample membership of mutiple clusters)</li>\n",
    "    <li>Cluster shape flexibility</li>\n",
    "</ol>\n",
    "\n",
    " <p>Disadvantages</p>\n",
    "    <ol>\n",
    "    <li>Sensitive for initislizing values</li>\n",
    "    <li>Convergence to a local optimum</li>\n",
    "    <li>Slow convergence rate</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster analysis\n",
    "<ol>\n",
    "    <li>Feature selection and extraction</li>\n",
    "    <li>Cluster algorithm selectiona nd tuning</li>\n",
    "    <li>Clustering validation</li>\n",
    "    <li>Results interpretation</li>\n",
    "   \n",
    "</ol>\n",
    "<p>cluster validation</p>\n",
    "<li>External indices:Adjusted rand index</li>\n",
    "<li>Internal indices:silohouette coeffient </li>\n",
    "<li> Relative indices --- compactness & separability</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: https://pdfs.semanticscholar.org/a630/316f9c98839098747007753a9bb6d05f752e.pdf, https://www.datasciencecentral.com/profiles/blogs/python-implementing-a-k-means-algorithm-with-sklearn,https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-clustering-and-different-methods-of-clustering/,http://www.web.stanford.edu/~hastie/Papers/gap.pdf,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
